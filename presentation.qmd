---
title: "Modern Machine Learning in R"
author: "Jakob Richter, Sebastian Fischer, mlr3 Team"
format:
  revealjs:
    execute:
        echo: true
    theme: [simple, style.scss]
    logo: "figures/logo.png"
    height: 1080
    width: 1980
    code-copy: false
    slide-number: c/t
    center-title-slide: false
    higlight-style: atom-one
    df-print: kable

---

```{r, include = FALSE}
library("mlr3verse")
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
set.seed(1)
```


## `mlr3` Ecosystem


```{r}
#| echo: false
#| fig-align: "center"
knitr::include_graphics("figures/mlr3_ecosystem.png")
```

## Penguins

```{r}
#| echo: false
#| fig-align: "center"
knitr::include_graphics("figures/penguins.png")
```
^[Source: https://github.com/allisonhorst/penguins]



## Penguins

- Our goal: Classify penguins into their species: Adelie, Chinstrap, Gentoo
- `penguins_simple` is a simplified version of the well known palmer penguins dataset

```{r}
penguins = mlr3data::penguins_simple
head(penguins)
```


## Classifying penguins without `mlr3`

- `R` gives you access to many machine learning methods
- ... but without a unified interface
- things like performance evaluation are cumbersome

```{r}
svm_model = e1071::svm(species ~ ., data = penguins)
```

vs.

```{r}
#| output: false
xgb_model = xgboost::xgboost(data = as.matrix(penguins[, -1]),
  label = penguins$species, nrounds = 10)
```

## So you want to do ML in R

::: frame
```{r}
library("mlr3")
```

Ingredients:

-   Data / Task

-   Learning Algorithms

-   Performance Evaluation

-   Performance Comparison
:::


## Creating an `mlr3::Task`

-   Tabular data

-   Features

-   Target / outcome to predict

    -   discrete for classification
    -   continuous for regression
    -   target determines the machine learning "Task"

```{r}
library("mlr3")

task = as_task_classif(penguins, target = "species", id = "penguins")

task
```


# Intermezzo: R6

## R6 -- All you need to know

::: frame
`mlr3` uses the *R6* class system. Some things may seem unusual if you see them for the first time.

<!---
Out because mlr3verse does not even export the classes, and there are much more practical functions for the end user available.


-   Objects are created using <Class>\$new().

```{r}
task = TaskClassif$new("penguins", penguins, "species")
```
--->

-   Objects have fields that contain information about the object.

```{r}
task$nrow
```

-   Objects have methods that are called like functions:

```{r}
task$filter(rows = 1:10)
```

-   Methods may change ("mutate") the object (reference semantics)!

```{r}
task$nrow
```
:::


# Sugar Functions and Dictionaries

## Dictionaries

::: frame
<!---
-   Ordinary constructors: `TaskClassif$new()` / `LearnerClassifRpart$new()`
--->
-   `mlr3` offers constructors for all R6 objects:

-   They access `Dictionary` of objects:

| Object       | Dictionary        | Short Form | Converter|
|:-------------|:------------------|:-----------|-----|
| `Task`       | `mlr_tasks`       | `tsk()`    | `as_task()`     |
| `Learner`    | `mlr_learners`    | `lrn()`    |  `as_learner()`    |
| `Measure`    | `mlr_measures`    | `msr()`    | `as_measure()`     |
| `Resampling` | `mlr_resamplings` | `rsmp()`   | `as_resampling()`     |

Dictionaries can get populated by add-on packages (e.g. `mlr3learners`)
:::

## Dictionaries

::: frame
```{r}
# list items
tsk()

# retrieve an element
task = tsk("penguins_simple")
task
```
:::

# Learning Algorithms

## Availabe `mlr3::Learner`s

- `as.data.table(<DICTIONARY>)` creates a data.table with metadata about objects in dictionaries:
- An overview of available objects can also be found on the `mlr3` website: https://mlr-org.com
- The `Learner`s are just wrappers around other packages

```{r}
mlr_learners_table = as.data.table(mlr_learners)
mlr_learners_table[1:10, c("key", "packages", "predict_types")]
```

## Learning Algorithms {.smaller}

-   Get a Learner provided by `mlr3`

```{r}
learner = lrn("classif.rpart")
```


-   Train the `Learner`

```{r}
learner$train(task)
```

-   The `$model` is the rpart model: a decision tree

```{r}
print(learner$model)
```

## Learning Algorithms

```{r, out.width = "50%"}
#| echo: false
#| fig-align: "center"
knitr::include_graphics("figures/mlr3book_figures-2.svg")
```

## Prediction

-   Our scientists have found two new penguins and want to know their species

    ```{r}
    #| echo: false
    new_penguins = data.frame(
      bill_depth = c(20, 21), bill_length = c(37, 39), body_mass = c(4000, 3750), flipper_length = c(281, 190),
      year = c(2007, 2006), island.Biscoe = c(0, 0), island.Dream = c(1, 0), island.Torgersen = c(0, 1),
      sex.female = c(1, 0), sex.male = c(0, 1)
    )
    new_penguins
    ```

-   To do so, we call the `$predict_newdata()` method using the new data:

```{r}
prediction = learner$predict_newdata(new_penguins)
```

-   We get a Prediction object:

```{r}
prediction
```


## Prediction

-   We can make the Learner predict *probabilities* when we set `predict_type`:

```{r}
learner$predict_type = "prob"
learner$predict_newdata(new_penguins)
```

## Success #1! 

```{r}
#| echo: false
#| fig-align: "right"
#| out.width: "30%"
knitr::include_graphics("figures/partying-face-emoji.svg")
```

We can now classify penguins into their species!
```{r}
l_rpart = lrn("classif.rpart", predict_type = "prob")
t_penguins = as_task_classif(penguins, target = "species", id = "penguins")
l_rpart$train(t_penguins)
new_penguin = data.frame(
  bill_depth = 20, bill_length = 37, body_mass = 4000, flipper_length = 281,
  year = 2007, island.Biscoe = 0, island.Dream = 1, island.Torgersen = 0,
  sex.female = 1, sex.male = 0
)
l_rpart$predict_newdata(new_penguin) |> as.data.table()
```

# Performance


## Performance Evaluation {.smaller}


- In order to evaluate the performance of our model we need to split our data into a training and a test set

```{r}
split = partition(task)
```

- We train our learner on the train set and make predictions for the test set

```{r}
learner$train(task, row_ids = split$train)
prediction = learner$predict(task, row_ids = split$test)
prediction
```

- We can evaluate the prediction using an `mlr3::Measure`

```{r}
measure = msr("classif.acc")
measure
prediction$score(measure)
```

## Performance Evaluation

```{r}
#| echo: false
#| fig-align: "center"
#| out-width: "80%"
knitr::include_graphics("figures/mlr3book_figures-3.svg")
```


## Performance Evaluation

- Because our dataset is very small, a simple train-test split is not enough to get a reliable estimate of the generalization error
- `mlr3` supports other resampling methods like k-fold cross-validation

# Resampling

## Resampling


:::: {.columns}

::: {.column width="50%"}

-   Resample description: How to split the data

    ```{r}
    cv3 = rsmp("cv", folds = 3)
    ```

-   Use the `resample()` function for resampling:

    ```{r}
    rr = resample(task, learner, cv3)
    ```

-   We get a `ResampleResult` object:

    ```{r}
    rr
    ```
:::

::: {.column width="50%"}

```{r}
#| echo: false
knitr::include_graphics("figures/mlr3book_figures-8.svg")
```

:::

::::

## Resample Result {.smaller}

-   Calculate performance:

    ```{r}
    rr$aggregate(msr("classif.acc"))
    ```

-   Predictions of individual folds

    ```{r}
    predictions = rr$predictions()
    predictions[[1]]
    ```

-   Score of individual folds

    ```{r}
    scores = rr$score(msr("classif.acc"))
    scores[1:3, c("iteration", "classif.acc")]
    ```

## Success #2! 

```{r}
#| echo: false
#| fig-align: "right"
#| out.width: "30%"
knitr::include_graphics("figures/partying-face-emoji.svg")
```

We can now access the performance of our model!
```{r}
l_rpart = lrn("classif.rpart")
t_penguins = as_task_classif(penguins, target = "species", id = "penguins")
rr = resample(t_penguins, l_rpart, rsmp("cv", folds = 3))
rr$aggregate(msr("classif.acc"))
```

## Performance Comparison {.smaller}

-   We now want to try a glmnet learner from the glmnet package and compare it with the classification tree

-   We also add a simple baseline learner that predicts the majority class

    ```{r}
    lrn_rpart = lrn("classif.rpart")
    lrn_glmnet = lrn("classif.glmnet")
    lrn_featureless = lrn("classif.featureless")
    ```

-   We create a benchmark design where each row specifies a resample experiment

    ```{r}
    design = benchmark_grid(
        tasks = task,
        learners = list(lrn_rpart, lrn_glmnet, lrn_featureless),
        resampling = rsmp("cv", folds = 3)
    )
    design
    ```

## Performance Comparison

-   We can run the benchmark experiment by calling `benchmark()` and obtain a `BenchmarkResult` object:

    ```{r}
    bmr = benchmark(design)
    bmr
    ```

-   We can compare the learners by aggregating the results:

    ```{r}
    bmr_ag = bmr$aggregate(msr("classif.acc"))
    bmr_ag[, c("task_id", "learner_id", "classif.acc")]
    ```

    $\Rightarrow$ `classif.glmnet` achieves the highest accuracy

## Visualizing Results

- The `mlr3viz` package contains `autoplot)` methods for many mlr3 objects

```{r}
#| out.width: "80%"
library(mlr3viz)
autoplot(bmr)
```

## Preprocessing

-   Earlier, we worked on a simplified version of the penguins dataset
-   We will now work with the original penguins data that contains missing values and that has categorical columns

    ```{r}
    task = tsk("penguins")
    task
    task$missings()
    ```

## Preprocessing

-   While some learners can deal with missing values and categoricals

    ```{r}
    lrn("classif.rpart")$properties
    ```
    others cannot:
    ```{r}
    lrn("classif.glmnet")$properties
    ```

- Within the `mlr3` ecosystem, we can assemble machine learning pipelines using `mlr3pipelines`


## Preprocessing

- To use the data with the  `lrn("classif.glmnet")` we will:

    - Scale the features
    - Handle missing values
    - Encode categorical columns


-   We can do this by combining `PipeOp`s in a `Graph`:

    ```{r}
    graph = po("scale") %>>%
      po("encode") %>>%
      po("imputemedian") %>>%
      lrn("classif.glmnet")
    graph
    ```

## Preprocessing

-   We can convert this `Graph` to a `GraphLearner`

    ```{r}
    lrn_glmnet_preproc = as_learner(graph)
    lrn_glmnet_preproc$properties
    ```

-   While the original glmnet learner fails on the new task

    ```{r}
    #| error: true
    lrn_glmnet$train(task)
    ```

    The preprocessed learner works

    ```{r}
    lrn_glmnet_preproc$train(task)
    ```
## Performance Evaluation on the New Task

Because the `GraphLearner` is also a `Learner`, we can evaluate the preprocessed learner just like before

```{r}
rr = resample(
    task,
    lrn_glmnet_preproc,
    rsmp("cv", folds = 3)
)
rr$aggregate(msr("classif.acc"))
```

## Success #3! 

```{r}
#| echo: false
#| fig-align: "right"
#| out.width: "30%"
knitr::include_graphics("figures/partying-face-emoji.svg")
```

We can now handle missing values and categorical columns for any learner!
```{r}
l_glment = lrn("classif.glmnet")
l_preproc = po("scale") %>>% po("encode") %>>% 
  po("imputemedian") %>>% l_glment |> as_learner()
t_penguins = as_task_classif(penguins, target = "species", id = "penguins")
rr = resample(t_penguins, l_preproc, rsmp("cv", folds = 3))
rr$aggregate(msr("classif.acc"))
```


## What is `mlr3pipelines`

Machine Learning Workflows:

- **Preprocessing**: Feature extraction, feature election, missing data imputation, ...
- **Ensemble methods**: Model averaging, model stacking
- `mlr3`: modular *model fitting*

$\Rightarrow$ `mlr3pipelines`: modular *ML workflows*


```{r, echo = FALSE, out.width="70%"}
knitr::include_graphics("figures/pipelines_12.png")
```

## Machine Learning Workflows


::: {.fragment fragment-index=1}
What do they look like?
![](figures/pipelines_6.png){.absolute top=450 left=400 width="50%"}
:::

::: {.fragment fragment-index=2}
- **Buildings block**: *what* is happening? $\rightarrow$ `PipeOp`
![](figures/pipelines_7.png){.absolute top=450 left=400 width="50%"}
:::

::: {.fragment fragment-index=3}
 - **Structure**: in what *sequence* is it happening? $\rightarrow$ `Graph`
![](figures/pipelines_8.png){.absolute top=450 left=400 width="50%"}
:::

::: {.fragment fragment-index=4}
  $\Rightarrow$ `Graph`: `PipeOp`s as **nodes** with **edges** (data flow) between them
![](figures/pipelines_8.png){.absolute top=450 left=400 width="50%"}
:::


# PipeOps

## PipeOp: Single Unit of Data Operation


:::{.fragment fragment-index=1}
* `pip = po("scale")` to construct

![](figures/pipeline-trim-34.png){.absolute top=400 left=0}
:::

:::{.fragment fragment-index=2}
* `pip$train()`: process data and create `pip$state`

![](figures/pipeline-trim-1.png){.absolute top=400 left=0}
:::

:::{.fragment fragment-index=3}
* `pip$predict()`: process data depending on the `pip$state`

![](figures/pipeline-trim-2.png){.absolute top=400 left=0}
:::


<!---
## PipeOp: Single Unit of Data Operation {.smaller}

```{r}
po_scale = po("scale")
trained = po_scale$train(list(task))
trained$output$head(3)
```

```{r}
head(po_scale$state, 2)
```

```{r}
po_scale$predict(list(task))[[1L]]$head(3)
```

## PipeOps

```{r}
mlr_pipeops$keys()
```

--->

## PipeOps

- Simple data preprocessing operations (scaling, Box Cox, Yeo Johnson, PCA, ICA)
- Missing value imputation (sampling, mean, median, mode, new level, ...)
- Feature selection (by name, by type, using filter methods)
- Categorical data encoding (one-hot, treatment, impact)
- Sampling (subsampling for speed, sampling for class balance)
- Ensemble methods on Predictions (weighted average, possibly learned weights)
- Branching (simultaneous branching, alternative branching)
- Combination of data
- Text processing
- Date processing
- **Soon**: building of neural networks as graphs in `mlr3torch`

## Preprocessing Pipeline

* `train()`ing: Data propagates and creates `$states`


:::{.fragment fragment-index=1}
![](figures/pipeline-trim-271.png){.absolute top=400 left=0}
:::

:::{.fragment fragment-index=2}
![](figures/pipeline-trim-272.png){.absolute top=400 left=0}
:::

:::{.fragment fragment-index=3}
![](figures/pipeline-trim-273.png){.absolute top=400 left=0}
:::

:::{.fragment fragment-index=4}
![](figures/pipeline-trim-274.png){.absolute top=400 left=0}
:::

:::{.fragment fragment-index=5}
![](figures/pipeline-trim-275.png){.absolute top=400 left=0}
:::

## Preprocessing Pipeline

* `train()`ing: Data propagates and creates `$states`
* `predict()`tion: Data propagates, uses `$states`

![](figures/pipeline-trim-276.png){.absolute top=400 left=0}

# Putting it all together

## Back to the Penguins Task

- Even though we have now already achieved good performance using the `lrn("classif.rpart")` learner,
  we want to see whether we can improve our performance by using a boosting algorithm and **tuning its hyperparameters**.

-   We have decided to go for XGBoost
    ```{r}
    lrn_xgb = lrn("classif.xgboost")
    ```

## Hyperparameters

-   Learners have *hyperparameters*

```{r}
as.data.table(lrn_xgb$param_set)[c(12,16,24,31), c(1:4,9,10,11)]
```

-   Changing them changes the `Learner`'s behavior

```{r}
lrn_xgb$param_set$set_values(
    nrounds = 10,
    eta = 0.1
)
```

## Tuning XGBoost

-   Instead of specifying hyperparameters to a given value we can also mark them for hyperparameter tuning

    ```{r}
    lrn_xgb$param_set$set_values(nrounds = to_tune(10, 100, logscale = TRUE))
    ```

-   Wondering which hyperparameters to tune? Check out [mlr3tuningspaces](https://github.com/mlr-org/mlr3tuningspaces)!

-   Because XGBoost does not support factor columns, we add a factor encoding preprocessing step

    ```{r}
    lrn_xgb = po("encode") %>>% lrn_xgb |> as_learner()
    lrn_xgb$properties
    ```

## Tuning XGBoost

-   Hyperparameter tuning is implemented in the `mlr3tuning` package:
    ```{r}
    library("mlr3tuning")
    ```

-   Besides knowing which hyperparameters to tune, we also need to specify:

    - The performance measure
    - The tuning method
    - The termination criterion
    - The resampling strategy





## Tuning XGBoost

We can tune the classification accuracy using random search with 20 evaluations and 3-fold CV:

```{r}
instance = tune(
    task = task,
    learner = lrn_xgb,
    measure = msr("classif.acc"),
    tuner = tnr("random_search"),
    term_evals = 20,
    resampling = rsmp("cv", folds = 3)
)
instance$result
instance$archive$data[1:3, 1:3]
```


## Tuning XGBoost

-   We can now train a learner on the full dataset using the optimal configuration
    ```{r}
    (optimal_nrounds = instance$result_learner_param_vals$classif.xgboost.nrounds)
    lrn_xgb$param_set$set_values(
      classif.xgboost.nrounds = optimal_nrounds
    )
    lrn_xgb$train(task)
    ```


## Tuning in `mlr3`

```{r}
#| echo: false
knitr::include_graphics("figures/mlr3book_figures-9.svg")
```

## AutoTuner

-   We can automate the process of first finding a good parameter configuration and then training the final model using an `AutoTuner`

    ```{r}
    lrn_xgb_tuned = auto_tuner(
      learner = lrn_xgb,
      measure = msr("classif.acc"),
      tuner = tnr("random_search"),
      term_evals = 20,
      resampling = rsmp("cv", folds = 3)
    )
    ```

-   Like for the `GraphLearner`, this can be used like any `mlr3::Learner` such as evaluating its generalization error using `resample()`, resulting in a nested resampling


## AutoTuner

```{r}
#| echo: false
#| fig-align: "center"
knitr::include_graphics("figures/mlr3book_figures-12.svg")
```

## Glmnet vs Tuned XGBoost
-   We change the `Learner`'s IDs:

```{r}
lrn_xgb_tuned$id = "tuned_xgboost"
lrn_glmnet_preproc$id = "glmnet"
```


-   We create a benchmark design to compare the tuned XGBoost pipeline with the glmnet pipeline

```{r}
design = benchmark_grid(
    task, 
    list(lrn_xgb_tuned, lrn_glmnet_preproc), 
    rsmp("cv", folds = 5))
bmr = benchmark(design)
bmr$aggregate(msr("classif.acc"))
```

## Glmnet vs Tuned XGBoost

```{r}
autoplot(bmr, measure = msr("classif.acc"))
```

## Success #4! 

```{r}
#| echo: false
#| fig-align: "right"
#| out.width: "30%"
knitr::include_graphics("figures/partying-face-emoji.svg")
```

We can now tune hyperparameters automatically!
```{r}
l_xgb = lrn("classif.xgboost", nrounds = to_tune(10, 100, logscale = TRUE))
l_xgb_pre = po("encode") %>>% l_xgb |> as_learner()
l_xgb_pre_auto = auto_tuner(
  learner = l_xgb_pre,
  measure = msr("classif.acc"),
  tuner = tnr("random_search"),
  term_evals = 10,
  resampling = rsmp("cv", folds = 3)
)
t_penguins = as_task_classif(penguins, target = "species", id = "penguins")
rr = resample(t_penguins, l_xgb_pre_auto, rsmp("cv", folds = 3))
rr$aggregate(msr("classif.acc"))
```

## Parallelizing Execution

- In real-world cases, hyperparameter tuning and benchmarking is computationally expensive
-   Fortunately, `mlr3` can be conveniently parallelized using the `future` package
    ```{r, eval = FALSE}
    future::plan("multicore")
    ```
- This allows to parallelize everything that internally calls `resample()` or `benchmark()`
- Submission on HPC clusters is supported via the `batchtools` connector `mlr3batchmark`

## We have written a book!

```{r}
#| echo: false
#| fig-align: "center"
knitr::include_graphics("figures/book-flyer.png")
```

## Want more?

### Resources

We have only shown you a glimpse of what is possible with `mlr3`!

- Read the `mlr3` book: [mlr3book.mlr-org.com](https://mlr3book.mlr-org.com/)
- Browse our website: [mlr-org.com](https://mlr-org.com)
- Read one of our gallery posts: [mlr-org.com/gallery.html](https://mlr-org.com/gallery.html)
- Check us out on GitHub: [github.com/mlr-org](https://github.com/mlr-org)
- This presentation: [github.com/sebffischer/mlr3_UseR_berlin_2023](https://github.com/sebffischer/mlr3_UseR_berlin_2023)

### Getting In Touch

- You can email us directly: [sebastian.fischer@stat.uni-muenchen.de](mailto:sebastian.fischer@stat.uni-muenchen.de), [code@jakob-r.de](mailto:code@jakob-r.de)
- You our publicly available mattermost channel: [lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/](https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/)
- **We are always looking for new contributors!**

